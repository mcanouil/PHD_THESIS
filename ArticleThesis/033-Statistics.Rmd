## Méthodes statistiques : des données à la biologie

### La statistique génétique
Les principes de l’héritabilité développés par Gregor Mendel (1822-1884) à la fin du XIXème siècle ont servi de fondements à la plupart des connaissances actuelles sur la transmission des traits des parents à leurs enfants. 
Au fur et à mesure du développement du concept de traits hérités s’est développée la génétique, la science qui étudie la transmission de ces traits et du matériel biologique dans les organismes vivants. 
La génétique est devenue partie intégrante de la recherche sur l’origine de certaines maladies, comme l’obésité et le diabète. 
Contrairement à l’étude des végétaux et des petits animaux (rat, souris, etc.), où la croissance et les croisements peuvent être contrôlés de façon expérimentale, et où la transmission des caractères étudiés est rendue possible dans un temps limité (en particulier grâce à un temps réduit de passage d’une génération à la suivante, p. ex. 2-4 mois pour le poisson-zèbre), la situation est plus complexe chez l’Homme, puisqu’il faut entre 20 et 30 ans pour qu’une nouvelle génération voit le jour.

Aujourd'hui, avec les évolutions technologiques au niveau des plateformes moléculaires, ayant notamment permis le séquençage du génome humain ("Human Genome Project" [@sawicki_human_1993]), le volume des données génomiques a augmenté au cours des dernières décennies et a ainsi permis le développement d'une branche de la statistique, soit la statistique génétique. 
Cette nouvelle branche vise à développer des outils d’analyses des caractères hérités et plus généralement des données génétiques, permettant en outre l’identification de facteurs de risque ou de déterminants génétiques pour des maladies complexes, tels que les cancers, les diabètes, les maladies cardio-vasculaires ou les troubles psychiatriques. 
Une maladie complexe est définie comme une pathologie dont les causes sont multiples, lesquelles peuvent être le fruit d’une interaction de facteurs comportementaux, environnementaux et génétiques pouvant produire un effet sur plusieurs gènes de façon simultanée. 
Ces maladies complexes s’opposent aux maladies dites mendéliennes, dont l’origine s’explique principalement par la génétique, via des processus de transmission conjecturés par Gregor Mendel. 
Ces maladies se caractérisent par la transmission d’un gène délétère à la descendance, comme c’est le cas pour les formes de diabète MODY discutées précédemment.


### Recueil et prétraitement des données
#### Puce-à-ADN
L’émergence des puces à ADN ("DNA micro-array"), notamment propulsées par les grands projets internationaux de séquençage tels le "Human Genome Project" [@sawicki_human_1993], "HapMap Project" [@gibbs_international_2003], "1\ 000 Genomes Project" [@siva_1000_2008; @the_1000_genomes_project_consortium_global_2015], a permis d'étendre le champ d'application de la statistique grâce à la disponibilité et la variété des données issue de ces puces, à savoir aussi bien des données de transcriptomique, de génomique et d'épigénomique. 
En effet, les puces à ADN utilisent le principe d'hybridation de l'ADN reposant sur la complémentarité de ces bases.
Rappelons le fonctionnement des puces à ADN d'un point de vue général.

(ref:Microarray) Schéma du protocole des puces à ADN.

```{r Microarray, fig.cap = '(ref:Microarray)', out.width = '6in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/Microarray.png")
```

L'ADN est extrait et purifié à partir d'un échantillon de tissu (p. ex. prélèvement sanguin ou salivaire). 
L'ADN purifié peut ensuite être amplifié au moyen d’une réaction en chaîne par polymérase (PCR), un procédé qui permet d'augmenter la quantité d'ADN. 
Un marquage des séquences par le remplacement de certaines bases nucléotidiques par leur analogue radioactif est ensuite réalisé. 
Les séquences d'ADN marquées sont ensuite hybridées sur des sondes spécifiques disposées sur les puces à ADN, puces pouvant contenir des milliers de ces sondes complémentaires de séquences d'ADN. 
Une fois l'hybridation des séquences d'intérêts réalisée, la puce est passée dans un outil de visualisation permettant de lire et quantifier la fluorescence émise par chaque puits (position d'une sonde sur la puce). 
Selon le type d'omique, le protocole utilisé dans les puces varie et peut inclure des étapes spécifiques, telles qu'une étape de conversion de l'ARN en cDNA (ADN complémentaire de l'ARN reconstitué par une étape de transcription inverse) pour une étude transcriptomique, ou bien une étape de bisulfitation dans le cas d'une étude méthylomique, permettant le changement de la cytosine non-méthylée d'un groupement CpG en uracile avant de passer à l'étape d'amplification PCR. 
C'est à l'issue de ces étapes (Figure \@ref(fig:Microarray)), que l'information génomique, transcriptomique ou méthylomique est disponible sous forme de données numériques pouvant être analysées après un prétraitement et un contrôle-qualité. 

#### Prétraitement
Comme dans toute analyse statistique, la validité des résultats est conditionnée par la qualité des données. 
De ce fait, une étape de contrôle de qualité et de plausibilité des données est indispensable. 
En plus des problématiques génériques telles que les valeurs extrêmes,  les études omiques soulèvent quelques niveaux de complexité supplémentaires provenant en grande partie des protocoles complexes générant ces données. 
Ainsi, dans le cas du génotypage, la qualité peut être influencée par plusieurs facteurs n'étant pas toujours sous contrôle, comme la qualité de l'ADN qui dépend du type de prélèvement de l'échantillon (p. ex. échantillon sanguin ou buccal, biopsie, etc.), le stockage et la conservation de l'échantillon (p. ex. température, stockage paraffine, etc.), et la plateforme de génotypage (c.-à-d. la technologie utilisée par le manufacturier).


##### Génomique
Dans les études populationnelles, les erreurs de génotypage survenant indépendamment du statut des individus (p. ex. malade/non malade) ou de leur génotype, peuvent occasionner une diminution de la puissance statistique sans pour autant modifier le risque de première espèce des tests [@fardo_quality_2009; @gordon_assessment_2001; @marquard_impact_2009], ce qui peut ne pas être le cas dans les études familiales où le taux de faux positifs peut alors être augmenté [@yan_impact_2016; @abecasis_impact_2001].
Cette diminution de la puissance statistique et augmentation de l'erreur de type 1 sont d'autant plus importantes lorsque les erreurs de génotypage se trouvent être associées aux génotypes et/ou phénotypes, voire au plan expérimental (c.-à-d. différents techniciens, séparation complète des groupes étudiés sur des puces différentes, etc.). 
Une étape de contrôle-qualité peut consister en l'application de plusieurs filtres successifs, en particulier au moyen du logiciel PLINK [@chang_second-generation_2015; @purcell_plink_2015]qui dispose de nombreuses fonctionnalités pour la manipulation de fichiers génomiques. 
Ces filtres peuvent être regroupés en deux catégories, d'une part sur les individus, et d'autre part sur les variants génétiques.

###### Contrôle-qualité des échantillons {#QCgenomique}


(ref:gendercheck) Le taux d'homozygotie (estimé à l'aide des variants du chromosome X) est représenté en fonction de la proportion de génotypes manquants par échantillon. Le taux d'homozygotie attendu pour les hommes est de 1 et inférieur à 0,2 pour les femmes. Les points rouges représentent les échantillons pour lesquels les informations sur le sexe sont discordantes ou manquantes.

```{r gendercheck, fig.cap = '(ref:gendercheck)', out.width = '3in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/gendercheck.png")
```

+ _Concordance du genre entre le génotype et le phénotype_ (Figure \@ref(fig:gendercheck))  
En mesurant le taux d'homozygotie au niveau des gonosomes pour chaque individu, il est possible de déterminer le sexe à partir du chromosome X. 
Ainsi, pour les femmes, qui présentent deux chromosomes X et peuvent donc avoir deux allèles différents pour chaque variant, le taux d'homozygotie doit être inférieur à 0,2. 
Pour les hommes, qui ne présentent qu'un seul chromosome X, le taux d'homozygotie attendu est de 1, ou au moins supérieur à 0,8 (seuil de tolérance). 
Ce filtre a deux objectifs\ : vérifier l'information du phénotype, et fournir une information quant à la qualité du génotypage.

(ref:samplecallrateevaluation) Distribution du taux de génotype manquant par échantillon.

```{r samplecallrateevaluation, fig.cap = '(ref:samplecallrateevaluation)', out.width = '3in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/samplecallrateevaluation.png")
```

+ _Taux de génotypage ou taux de génotype manquants_ (Figure \@ref(fig:samplecallrateevaluation))  
Cette vérification permet également d'identifier les individus pour lesquels un problème est survenu lors du génotypage ou de l'extraction d'ADN, en particulier un défaut de qualité de l'ADN. 
Les individus présentant plus de 5\ % de génotypes manquants sont généralement exclus à ce stade.

(ref:heterozygositycheck) Taux d'hétérozygotie par échantillon par rapport au taux de génotypage. À gauche, le taux d'hétérozygotie pour l'ensemble des variants, à droite, le taux d'hétérozygotie des variants dont la fréquence allélique est inférieure à 1\ %. Les lignes rouges horizontales représentent l'intervalle à plus ou moins quatre fois l'écart-type du taux moyen d'hétérozygotie. La ligne rouge verticale indique le seuil du taux de génotypage.

```{r heterozygositycheck, fig.cap = '(ref:heterozygositycheck)', out.width = '6in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/heterozygositycheck.png")
```

+ _Taux d'hétérozygotie (autosomes)_ (Figure \@ref(fig:heterozygositycheck))  
L'objectif de cette vérification est de s'assurer d'une qualité homogène des génotypes des individus. 
La distribution du taux d'hétérozygotie (hors chromosomes sexuels) chez tous les individus doit être inspectée pour identifier les individus ayant une proportion excessive ou réduite de génotypes hétérozygotes. 
En effet, cela peut respectivement indiquer une contamination (p. ex. mélange de deux ADN), ou une consanguinité au sein des échantillons d'ADN. 
Les individus présentant un taux d'hétérozygotie extrême par rapport à la distribution de celui-ci sur l'ensemble des individus sont exclus, généralement sur la base d'un écart à la moyenne de trois à quatre fois l'écart-type. 
Le taux d'hétérozygotie (donnée par $\frac{(n-h)}{n}$, où $n$ est le nombre de génotypes total observé et $h$ est le nombre de génotypes homozygotes observé pour un individu donné) différera selon les populations étudiées et selon l'ensemble des SNPs ciblés par une puce donnée.

+ _Degré d'apparentement (étude non-familiale)_  
Dans le contexte des études populationnelles d'association pangénomiques, les individus étudiés sont sélectionnés pour satisfaire un critère de non-apparentement en plus de critères purement liés aux hypothèses auxquelles l'étude doit répondre. 
En effet, la présence d'individus apparentés (c.-à-d. du second degré ou plus proche, ou ayant plus de 20\ % de leur génome parfaitement identique), voire d’individus en doublons, peuvent introduire un biais dans l'étude par la surreprésentation de génotypes spécifiques à quelques familles, et conséquemment modifier les fréquences alléliques qui ne seront alors plus représentatives de la population étudiée.
Pour éviter ce biais, le degré d'apparentement de chaque paire d'individus est mesuré à partir de la proportion de leurs génomes partagés avec un ancêtre commun (identité par descendance ou IBD). 
De cette façon, les individus en doublons ou jumeaux (monozygotes) présenteront un $IBD\simeq 1$, un $IBD\simeq 0,5$ pour les individus ayant un lien du premier degré (p. ex. parents, enfants, frères et sœurs) et un $IBD\simeq 0,25$ pour un lien du second degré (p. ex. oncles, tantes, grand-parents, etc.).
En raison d’erreur de génotypage, de stratification cachée dans l’échantillon (p. ex. due à différentes origines ethniques) ou de déséquilibre de liaison, ces valeurs d'IBD théoriques peuvent varier avec des données réelles et des intervalles de valeurs sont alors tolérés\ : par exemple, $[0,20\ ; 0,30]$ pour le premier degré, ou $[0,40\ ; 0,60]$ pour le second degré.

(ref:population) Premier plan factoriel de l'analyse en composante principale du jeu de données combinant la population d'étude et celle de référence (1\ 000 génomes). Avec à gauche la population de référence et à droite la population d'étude.

```{r population, fig.cap = '(ref:population)', out.width = '6in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/population.png")
```

+ _Stratification de la population_ (Figure \@ref(fig:population))  
Comme évoqué précédemment, une stratification peut exister au sein de la population d'étude, créée par des individus d'origines ethniques différentes ou de zones géographiques différentes, et peut induire un biais dans les résultats lors de l'analyse [@clayton_population_2005; @cardon_population_2003], en particulier si cette stratification n'est pas la même entre les sous-groupes formés des cas et des témoins.
L'approche la plus courante pour identifier une stratification demeure l'analyse en composantes principales ou ACP [@caussinus_models_1986; @patterson_population_2006; @price_principal_2006].
L'ACP est une méthode statistique multivariée qui, à partir d'une matrice contenant l'ensemble des observations (dans notre cas, les individus génotypés) sur un nombre $N$ de variables potentiellement corrélées (c.-à-d. les SNPs), vise à obtenir un nombre réduit  $n<N$ de composantes principales non corrélées et orthogonales. 
Les composantes sont calculées de sorte que la part de variabilité qu'elles peuvent expliquer décroisse de la première à la dernière composante. 
Afin d'évaluer une potentielle stratification d'origine ethnique, la matrice des génotypes est augmentée des génotypes provenant d'une base de référence [@gibbs_international_2003; @siva_1000_2008; @the_1000_genomes_project_consortium_global_2015].
Ces bases de références contiennent des individus dont l’origine ethnique a été vérifiée par génotypage ou séquençage.
L'ACP est alors réalisée sur un jeu de données comportant les populations de références et l’échantillon étudié.
En raison de la grande diversité génétique observée entre individus d'origines caucasiennes, africaines et asiatiques, les deux premières composantes sont généralement suffisantes pour identifier une stratification ethnique dans l’échantillon.

\clearpage

###### Contrôle-qualité des SNPs

(ref:variantbasedQC) Taux de génotypage par variant. La ligne rouge indique le seuil de 98\ %.

```{r variantbasedQC, fig.cap = '(ref:variantbasedQC)', out.width = '3in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/variantbasedQC.png")
```

+ _Taux de génotypage ou taux de génotypes manquants_ (Figure \@ref(fig:variantbasedQC))  
Sur le même principe que le taux de génotypage pour un individu, le taux de génotypage d’un SNP est examiné. 
Un taux de succès, généralement fixé à 95\ %, est toléré, seuil en dessous duquel le SNP sera exclus des analyses.

(ref:hardyweinbergpvalues) Distribution des valeurs-p du test des variants à l'équilibre de Hardy-Weinberg (HWE). La ligne rouge horizontale indique le seuil de significativité pour $\alpha=0,0001$.

```{r hardyweinbergpvalues, fig.cap = '(ref:hardyweinbergpvalues)', out.width = '3in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/hardyweinbergpvalues.png")
```

+ _Équilibre de Hardy-Weinberg_ (Figure \@ref(fig:hardyweinbergpvalues))  
L'équilibre de Hardy-Weinberg (HWE) constitue l'un des principes fondamentaux de la génétique des populations. 
Pour une population suffisamment grande, non apparentée (c.-à-d. population panmictique où les accouplements se font au hasard ou de façon équiprobable), sans pression de sélection, et lorsque les générations d'individus successives sont discrètes et séparées, cet équilibre prédit que les proportions génotypiques d'un variant donné restent constantes d’une génération à la suivante et s’écrivent simplement comme le produit mathématique des fréquences alléliques de cette population. 
Une forte déviation par rapport à l'HWE est un motif d'exclusion d’un SNP dans les études associations pangénomiques, car peu probable et sans doute révélatrice d'une erreur de génotypage. 
Mais un écart important par rapport à l'HWE peut également indiquer un effet de sélection, c'est-à-dire que les cas (dans une étude cas/témoin) peuvent montrer une déviation à l'HWE pour des loci associés à la maladie étudiée\ : exclure ces loci reviendrait donc à exclure ce qui est précisément l’objet de l'étude  [@wittke-thompson_rational_2005]. 
Ceci explique pourquoi le seuil de significativité du test d'écart à l'HWE varie d’une étude à l’autre, bien que ce test ne soit effectué que dans le groupe témoin [@meyre_genome-wide_2009; @sladek_genome-wide_2007; @burton_genome-wide_2007].

(ref:mafdistribution) Répartition du nombre de SNP par classe de fréquence des allèles mineurs (MAF).

```{r mafdistribution, fig.cap = '(ref:mafdistribution)', out.width = '3in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/mafdistribution.png")
```

+ _Fréquence allélique mineure (en anglais, maf)_ (Figure \@ref(fig:mafdistribution))  
Un filtre sur la fréquence allélique est appliqué pour ne conserver dans les analyses statistiques que les polymorphismes dont la fréquence de l'allèle mineur est supérieure à 5\ % (par définition, cette fréquence est comprise entre 0 et 50\ %). 
Dans certaines études, pour conserver des SNPs considérés comme "rares", c.-à-d. dont la maf estinférieure à 5\ %, il est possible d'augmenter le seuil du taux de génotypage par SNP [@burton_genome-wide_2007].
Cependant, les résultats des tests d'associations observés pour ces SNPs rares sont moins robustes, malgré un taux de génotypage plus élevé (p. ex. 99\ %), principalement parce que ces résultats peuvent être produits par les génotypes rares de quelques individus seulement. 
En effet, @morris_evaluation_2010 ont montré que la puissance statistique pour détecter des associations pour des SNPs rares était faible, particulièrement avec des approches dites "_simple SNP_", c.-à-d. un SNP à la fois. En réalité, leur exclusion n'aurait qu'un impact modéré sur les résultats de l'étude.

En conclusion, même après avoir appliqué ces différents filtres de contrôle-qualité, aussi bien au niveau des individus qu’au niveau des SNPs, des erreurs de génotypage peuvent subsister, d’où la nécessité de répliquer les associations détectées dans d’autres échantillons.

\clearpage

##### Transcriptomique {#transcriptomiquePT}
Les données de transcriptomique provenant de la lecture et de la quantification de la fluorescence d'une puce à ADN nécessitent également un prétraitement afin de garantir la validité et la fiabilité de celles-ci.
Avant la réalisation d'une étude transcriptomique, une considération particulière doit être prise quant à la conception du plan d'expérience pour réduire les biais techniques, p. ex. en équilibrant les échantillons sur les puces et plaques, en réalisant l'expérience en un minimum de temps, ou en limitant le nombre d'expérimentateurs [@quackenbush_microarray_2002]), et ainsi permettre un contrôle-qualité plus efficace, particulièrement lors de la normalisation des données.

Les plateformes qui permettent de quantifier l'expression des gènes via la quantification d'ADN complémentaire (cDNA) à partir de séquences d'ARN (mRNA, microRNA) ne font pas appel aux mêmes techniques.
Plusieurs outils ont été développés permettant l'importation et le prétraitement des données brutes directement depuis le logiciel statistique R [@R-limma; @R-AgiMicroRna].

+ _Valeur-p de détection_  
Selon la plateforme utilisée (principalement chez Illumina), la mesure brute d'expression peut être accompagnée d'une valeur-p de détection calculée à partir de la mesure d'intensité de sondes contrôles, permettant d'évaluer si le signal observé est statistiquement différent de l'intensité (artéfactuelle) observée au niveau des sondes contrôles.
Cette mesure peut alors être utilisée en tant que filtre en amont des étapes de normalisation pour exclure les sondes non-détectées (à partir d'un seuil, p. ex. $\textrm{valeur-p}<0,05$) sur un nombre suffisant déterminé par l'expérimentateur ou analyste (p. ex. sonde détectée sur 95\ % des échantillons).

+ _Correction du "bruit de fond"_  
En effet, après que les puces à ADN aient été scannées pour évaluer la fluorescence des sondes permettant la quantification indirecte de l'ARN, deux types de mesures sont disponibles\ : l'intensité de fluorescence au niveau d'un puits (une sonde par puits) et l'intensité de fluorescence ambiante (au voisinage des puits).
Cette seconde information, appelée "bruit de fond", doit être prise en compte.
Plusieurs approches sont disponibles pour réaliser cette correction de l'intensité des sondes (signal) par l'intensité ambiante, dont l'approche la plus classique consiste à soustraire l'intensité ambiante au signal.
Cependant, cette correction produit des effets indésirables, puisqu'elle peut générer des valeurs négatives lorsque l'intensité ambiante est plus forte que le signal ce qui, lors du passage au logarithme ou logarithme-ratios, aboutissent à la génération de données manquantes, rendant de ce fait inexploitable ces mesures.
L'extension R _limma_ [@R-limma] propose plusieurs méthodes pour cette correction du "bruit de fond", dont une approche basée sur un modèle de convolution normale + exponentielle.
Le modèle suppose que les intensités observées sont la somme de l'intensité ambiante et du signal, l'intensité ambiante suivant une distribution normale lorsque le signal suit une distribution exponentielle [@irizarry_exploration_2003; @silver_microarray_2009; @ritchie_comparison_2007].
Cette méthode permet de garantir que le signal de l'ensemble des sondes est strictement positif, et ainsi permet le passage au logarithme sans perte de données.

(ref:normalisation) Données d'expression avant et après normalisation des intensités.

```{r normalisation, fig.cap = '(ref:normalisation)', out.width = '4in', out.height = '3.25in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/normalisation.png")
```

+ _Normalisation inter-puces_  
Une étape de normalisation est réalisée afin de repérer et corriger un effet, ou biais, systématique sur l'ensemble des mesures d'expression pouvant correspondre à un effet puce, c'est-à-dire qu'il peut exister une différence dans l'application des protocoles d'une puce à l'autre induisant des mesures systématiquement plus élevées sur une puce en particulier.
Ces différences techniques peuvent être le résultat d'une mauvaise hybridation des cDNA sur une puce, ou d'une faible quantité de cDNA aboutissant à des pics de fluorescence plus faibles dans ces cas-là (Figure \@ref(fig:normalisation)).
La principale méthode de normalisation utilisée dans les études transcriptomiques est la normalisation quantile, permettant de rendre similaire, d'un point de vue statistique, deux ou plusieurs distributions.
Autrement dit, la distribution des mesures d'expression d'une puce est prise en référence, et la distribution des mesures obtenues sur une seconde puce est normalisée pour que celle-ci soit comparable à la première.
Dans le même temps, la normalité des mesures d'expression n'étant pas toujours vérifiée, une transformation logarithmique (base 2) est préalablement appliquée sur les données brutes, ou sur les ratios des mesures obtenues\ : pour une sonde dans une condition donnée, sa mesure est divisée par la mesure obtenue pour la même sonde dans une condition contrôle, comme cela est fait dans les expériences de quantification par RT-PCR, permettant ainsi la quantification (fluorescence) d'un mRNA spécifique via la transcription inverse suivie d'une amplification des fragments de cDNA.

+ _Filtre des sondes_  
Une fois les données normalisées, une étape additionnelle peut être appliquée pour filtrer les sondes, par exemple, pour ne conserver que les sondes exprimées (à partir d’un seuil défini au préalable, selon un gène de ménage servant de référence, c'est-à-dire un gène dont le niveau d'expression est constant dans l'ensemble des tissus) sur un certain nombre d'échantillons d'une condition (p. ex. 95\ % des individus contrôles).

+ _Profil extrême global_ (Figure \@ref(fig:pcaexpr))  
Enfin, un dernier contrôle consiste à la vérification et à l'identification d'individus présentant un profil transcriptomique extrême, par exemple, au moyen d'une ACP.
D’une part, l'ACP permettra de pouvoir identifier une éventuelle stratification, au sein des individus, associée ou non aux conditions expérimentales\ ; d'autre part, elle permettra d'identifier des individus extrêmes par rapport à l'ensemble des conditions ou d'une condition expérimentale donnée, qui pourrait être liée à la quantité de cDNA ou à la qualité d'ARN.

(ref:pcaexpr) Identification de profil extrême à partir des premières composantes de l'analyse en composante principale.

```{r pcaexpr, fig.cap = '(ref:pcaexpr)', out.width = "6in", include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/pcaexpr.png")
```

Les puces à ADN utilisées en transcriptomique n'exploitent pas toutes les mêmes techniques expérimentales (p. ex. puce monochrome ou bi-couleur), nécessitant de ce fait d'adapter les étapes de prétraitement et de contrôle-qualité décrites précédemment. 

\clearpage

##### Méthylomique {#methylomiquePT}
Comme les précédentes techniques omiques, les données de méthylomique doivent faire l'objet d'un prétraitement et d'un contrôle-qualité.
Les principes et techniques évoqués pour la génomique et la transcriptomique peuvent et sont employés également en méthylomique.
Néanmoins, d'autres vérifications et certaines adaptations méthodologiques sont nécessaires.
En génomique, les valeurs sont discrètes et codées 0, 1, 2 (modèle additif)\ ; en transcriptomique, les valeurs (brutes) sont continues et définies sur l’intervalle $[0, +\infty($, tandis qu’en méthylomique, les données sont bornées entre 0 et 1.

(ref:Illumina450k) Représentation des techniques de marquage utilisées sur la puce Illumina Infinium HumanMethylation450 [@maksimovic_swan:_2012]. a) Infinium I. Chaque CpG est interrogé à l'aide de deux types de sondes\ : méthylée (M) et non-méthylée (U). Les deux types de sondes incorporent le même nucléotide marqué pour un site CpG donné, produisant ainsi la même fluorescence. Le nucléotide qui est ajouté est déterminé par la base en aval du "C" du CpG ciblé. Le pourcentage de méthylation peut être calculé en comparant les intensités des deux sondes (U et M) de la même couleur. b) Infinium II. Chaque CpG est interrogé à l'aide d'un seul type de sonde. L'état de méthylation est détecté par la base complémentaire unique à la position du "C" du CpG ciblé, ce qui entraîne l'ajout d'un nucléotide marqué "G" ou "A", complémentaire à la cytosine (méthylé) ou à la thymine (non-méthylé), respectivement. Le pourcentage de méthylation du site CpG ciblé est déterminé en comparant les intensités des deux couleurs émises par les nucléotides marqués.

```{r Illumina450k, fig.cap = '(ref:Illumina450k)', out.width = '6in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/Illumina450k.png")
```

Les étapes décrites ci-après, quoique non-exclusives ou exhaustives, concernent principalement la puce Illumina HumanMethylation450 [@bibikova_high_2011], puce qui a été utilisée dans l'article du Chapitre \@ref(Article3).
La puce Illumina HumanMethylation450 permet l'identification de la méthylation sur plus de 450\ 000 sites CpG localisés sur l'ensemble du génome, et se caractérise par l'utilisation de deux processus d'analyses chimiques différents (Infinium I et II).
L'Infinium I utilise un système de marquage fluorescent monocouleur, tandis que l'Infinium II exploite un système bi-couleur pour quantifier la méthylation.
À cela, s'ajoute un second niveau de différences, puisque l'Infinium I dispose de deux types de sondes pour identifier les allèles méthylés et les allèles non-méthylés.
L'Infinium II n'utilise qu'un seul type de sonde qui, lors de l'hybridation des fragments d'ADN sur la puce, rend accessible ou non la base nucléotidique complémentaire à celles marquées (T et G pour les sites CpG respectivement non-méthylés et méthylés) (Figure \@ref(fig:Illumina450k)).
@dedeurwaerder_evaluation_2011 ont montré que ces différentes techniques (c.-à-d. réactions chimiques et types de sonde) impactaient directement les résultats obtenus.
Les extensions R et les algorithmes de prétraitement des données de méthylation se sont fortement développés et multipliés ces dernières années, imposant à l'utilisateur la délicate tâche de sélectionner les méthodes les plus efficaces et les plus adaptées à ces données.

###### Contrôle-qualité des sites CpG

+ _Filtre des sites_   
En amont, un premier filtre est réalisé pour exclure les sites de méthylation pouvant présenter des résultats étranges, principalement pour des raisons techniques\ :

    - Sondes trans-réactives  
La transformation des cytosines non-méthylées en thymine lors de la bisulfitation entraîne un changement dans la distribution des quatre bases nucléotidiques dans le génome, et par conséquent augmente la probabilité que les sondes Infinium puissent s'hybrider sur d'autres portions du génome que celles ciblées initialement.
La méthylation observée sur ces sites peut donc être le résultat d'un mélange de la méthylation du site cible et de celle d'autres sites.
Des annotations des sites supposés ou vérifiés comme non-spécifiques ont été générées pour permettre de les identifier et/ou de les exclure [@price_additional_2013; @zhang_analysis_2012; @chen_discovery_2013].

    - Sondes incluant un SNP  
Il peut être nécessaire d'exclure les sondes comportant un SNP, puisque la quantification de la méthylation est basée sur le génotypage (quantitatif) de C/T (après conversion bisulfite) [@price_additional_2013; @chen_discovery_2013].
En effet, des polymorphisme C/T peuvent être présents naturellement chez un individu, et ainsi être considérés comme un résultat de la conversion bisulfite.
La séquence d'ADN est alors confondue avec la méthylation.
Par exemple, un individu homozygote C/C aura une méthylation proche de 100\ %, pendant qu’un individu homozygote T/T aura quant à lui une méthylation de 0\ %. Enfin, un individu hétérozygote C/T sera à 50\ % méthylé sur ce site.
En l'absence de données de génomique conjointement aux données de méthylomique, il est préférable d'exclure ces sondes et les sites correspondants avant analyse.

    - Valeur-p de détection  
Tout comme pour les puces d'expression d'Illumina, les puces de méthylation fournissent, en plus de la quantification de la méthylation, des valeurs-p de détection pour l'ensemble des sondes/sites basées sur des sondes contrôles.
Par exemple, dans l'étude présentée au Chapitre \@ref(Article3)), une méthode dérivée du contrôle-qualité appliqué en génomique a été utilisée.
Ainsi, un site est exclu dès lors que la valeur-p de détection est supérieure à $10^{-6}$ pour au moins 5\ % des échantillons.
Un filtre équivalent est appliqué sur les échantillons, à savoir qu'un échantillon doit présenter des valeurs-p de détection inférieures à $10^{-6}$ pour plus de 75\ % des sites pour être conservé.

+ _Normalisation_  

    - Intra-puce\ : Infinium I/II  
Une première normalisation est nécessaire pour corriger les différences de distribution des niveaux de méthylation entre Infinium I et Infinium II. 
Cette étape de normalisation Infinium I/II est indispensable\ ; cependant, il convient de choisir la bonne méthode après avoir examiné soigneusement la distribution des valeurs de méthylation [@marabita_evaluation_2013; @yousefi_considerations_2013].  
Plusieurs méthodes ont été développées\ :  

        - "Peak Based Correction" (PBC) [@dedeurwaerder_evaluation_2011]\ : la distribution bimodale des valeurs de méthylation, avec un pic pour les sites non-méthylés et un pic pour les sites méthylés, de l'Infinium I est utilisée comme référence pour fixer les deux modes des valeurs de méthylation de l'Infinium II.
        
        - "Subset quantile Within-Array Normalisation" (SWAN) [@maksimovic_swan:_2012]\ : une normalisation quantile est appliquée à une sélection aléatoire de sondes Infinium I et Infinium II (dont la composition en nombre de sites CpG est similaire et par type de sondes de l'Infinium II). Les distributions (similaires) ainsi obtenues sont ensuite utilisées pour ajuster (interpolation linéaire) les valeurs des sondes restantes. Similaire à l'approche SWAN, l'approche "Subset Quantile Normalisation" (SQN) [@touleimat_complete_2012] ne diffère que par la classification des sondes, effectuée selon leur position par rapport aux îlots CpG.
        
        - "Beta-Mixture Quantile Normalisation" (BMIQ) [@teschendorff_beta-mixture_2013]\ : la densité de distribution des valeurs de méthylation des Infinium I et II est chacune décomposée en un mélange de trois distributions Bêta, où chaque distribution Bêta correspond à un état de méthylation\ : non-méthylé (proche de 0\ %), hémi-méthylé (proche de 50\ %), et méthylé (proche de 100\ %). Une normalisation quantile est ensuite appliquée entre chaque distribution Bêta de l'Infinium II et la distribution Bêta correspondante de l'Infinium I (Figure \@ref(fig:BMIQ)).

    - Intra-puce\ : "bruit de fond"  
Une correction du "bruit de fond" suivant les mêmes stratégies que celles appliquées pour les puces d'expression (p. ex. modèle par convolution Normale + Exponentielle) [@xu_enmix:_2015; @triche_low-level_2013].

    - Inter-puce  
Une étape supplémentaire de normalisation peut être appliquée pour réduire autant que possible les effets "plaques" et "puces".
Des méthodes ont été développées dans cet objectif, comme la méthode _ComBat_ [@johnson_adjusting_2007; @leek_sva_2012]), ayant déjà montré son efficacité sur des données de méthylation [@sun_batch_2011; @leek_tackling_2010].
Cependant, il convient de faire attention à l'utilisation de ces méthodes, notamment lorsqu'elles permettent d'inclure des informations sur les conditions biologiques testées (p. ex. groupes cas et contrôle) [@nygaard_methods_2015].
Il est également important de garder à l'esprit que la meilleure façon d'éviter les problèmes liés aux effets "plaques" et "puces" est d'avoir réalisé un plan d'expérience ex ante, c'est-à-dire d'avoir prévu la répartition des échantillons et conditions de façon homogène entre les puces.

(ref:BMIQ) Distribution des valeurs-$\beta$ de méthylation des sondes Infinium I et II, avant normalisation BMIQ [@teschendorff_beta-mixture_2013] (à gauche) et après normalisation BMIQ (à droite).

```{r BMIQ, fig.cap = '(ref:BMIQ)', out.width = '6in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/InfiniumType.png")
```

###### Contrôle-qualité des échantillons
Un premier contrôle des échantillons est réalisé lors de l'application du filtre sur les valeurs-p de détection, permettant ainsi d'exclure les individus ayant une faible qualité de détection,  résultant d'une dégradation de l'ADN, par exemple.
De la même façon qu'en génomique et transcriptomique, une ACP peut être réalisée dans le but d'identifier des échantillons dont le profil de méthylation est extrême, et identifier une stratification des données qui pourrait être liée à un biais technique non pris en compte.


### Analyses omique et multi-omique

#### Génomique

##### Avant l’ère des études d'association pangénomiques

En l'absence de données omiques, et plus particulièrement de données génomiques, les études d'_agrégation_, d'_héritabilité_ et/ou de _ségrégation_ oont permis de mettre en avant la contribution de la génétique à de nombreuses pathologies, en se basant sur l'apparentement d'individus provenant d'une même famille.
Ces études sont réalisées dans un certain ordre, consistant à évaluer en premier lieu, le caractère héritable d'un trait (binaire ou quantitatif), et en second lieu, le mode de transmission génétique d’une génération à la suivante.

+ •	Les études d'agrégation (pour traits binaires, par exemple, le statut diabétique [@bijanzadeh_recurrence_2017; @weijnen_risk_2002]) ont pour but d'évaluer le risque relatif d'un trait entre les membres d'une famille [@laird_15_2000]. 
Dans ces études, un individu porteur de la pathologie est dit "proband".
Même si ces études permettent de montrer qu'un certain trait peut se retrouver de façon plus importante dans une famille plutôt que dans une autre (c.-à-d. agrégation), elles ne permettent pas d'éliminer un potentiel effet de confusion produit par l'environnement, puisque celui-ci peut être identique pour tous les individus d'une même famille.

+ Les études d'héritabilité (pour traits quantitatifs, par exemple, la glycémie à jeun) ressemblent aux études d'agrégation, et cherchent à évaluer l'effet de la génétique dans la variabilité d'un trait quantitatif.
On parle de cet effet comme l'héritabilité d'un trait, laquelle donne la proportion de la variabilité totale du trait qui est expliquée par l'ensemble des variations liées à la génétique.

+ Les études de ségrégation ont pour objectif d'identifier le mode de transmission génétique d'un trait, à savoir si le gène de susceptibilité suit un modèle génétique de type récessif/dominant, additif ou de codominance.
Ces différents modes de transmission mendélienne s'illustrent bien en utilisant le groupe sanguin ABO (phénotype), où les allèles sont définis par A, B et O.
L'allèle O est l'allèle récessif par rapport aux allèles A et B\ : ainsi, un génotype A/O donnera le groupe sanguin A, tandis que le génotype B/O donnera le groupe sanguin B.
Le groupe sanguin AB est issu de la codominance des allèles A et B.
Enfin, dû au caractère récessif de l'allèle O, un individu doit être porteur de deux allèles O (génotype O/O) pour présenter un groupe sanguin O.

Ces différentes études ont initialement été développées lorsque les techniques de génotypage étaient onéreuses, et ont servi en quelque sorte d’études préliminaires aux études d'association pangénomiques.

##### Les études d'association pangénomiques
Les études d'association génétiques se sont développées au cours des 20 dernières années, et viennent compléter les études de liaison qui quantifie l'excès d'allèles transmis par descendance (selon les principes mendéliens) entre des individus apparentés (principalement, du premier et second degré).
Nous distinguerons deux types d'études d'association\ : d'une part, lorsque l'analyse porte sur des familles, et d'autre part, lorsque l'analyse porte sur des individus non apparentés.
Dans le cas des études familiales, en particulier des trios correspondant à un enfant atteint/porteur et de ses deux parents, le principal test employé est le test du déséquilibre de transmission (TDT).
Ce test se base sur une statistique calculée par $(t-u)^2 / (t+u)$, où $t$ le nombre d'allèles transmis et $u$ le nombre d'allèles non transmis.
Sous l'hypothèse nulle d'indépendance des distributions de $t$ et $u$, cette statistique suit une distribution du $\chi^2$ à 1 degré de liberté.
Dans le contexte des études familiales, ce test confère une puissance statistique plus importante que les tests d'association classiques avec design cas/témoins, et limite l’effet lié à une stratification de la population d'étude, notamment en faisant l'hypothèse d'une homogénéité phénotypique intra-famille plutôt qu'inter-famille. 

###### L'approche dite "classique"
Les études d'association pangénomiques (GWAS), dont la toute première a été réalisée en 2005 [@klein_complement_2005], et la première dans le diabète de type 2 en 2007 [@sladek_genome-wide_2007], ont été le principal moteur dans la découverte de nombreux loci de susceptibilité à diverses maladies complexes, et sont répertoriées au sein de la base de données "GWAS Catalog" [@macarthur_new_2017].
Deux raisons principales expliquent cet essor\ : la première est la limite de détection du TDT dans les maladies complexes (non monogéniques), où l'excès de transmission de chaque allèle contribuant à cette maladie est modéré voire faible.
La seconde raison concerne les études de puissance réalisées au début des années 2000 [@risch_future_1996; @sham_power_2000] qui suggéraient un gain de puissance statistique pour détecter des associations dans les études d'association comparativement aux études de liaison.
Cependant, ce gain de puissance demeure relatif, puisque celui-ci dépend de la fréquence allélique ou encore du déséquilibre de liaison entre les loci étudiés [@tu_power_1999].

Une étude GWA consiste généralement en l'application d'un modèle de régression logistique ou linéaire généralisé, selon la nature du trait étudié, à l'ensemble des polymorphismes identifiés via une puce ADN.
Le nombre de SNPs est passé de près de 300\ 000 à plusieurs millions, en particulier grâce à des techniques d'imputation du génome [@howie_genotype_2011] à l'aide de génome de référence [@the_1000_genomes_project_consortium_global_2015], permettant de ce fait de pallier aux spécificités des différentes puces, et d'accroître d’autant la quantité d'information génétique disponible pour chaque individu.
`r numberEq.docx("g[E(\\boldsymbol{Y})]=\\beta_0 + \\beta_1 \\boldsymbol{X} + \\beta \\boldsymbol{Z}", "eq:GWAS")`
Le modèle classique de régression donné par l’Équation `r labEq.docx("GWAS")` consiste à expliquer un trait $Y$, par exemple le statut DT2 (binaire) ou la glycémie à jeun (quantitatif), par des SNPs avec en général une hypothèse d'additivité des allèles.
Ainsi le génotype $X$ est codé 0 pour le génotype homozygote majeur (c.-à-d. homozygote pour l'allèle avec la plus forte fréquence allélique), 1 pour le génotype hétérozygote et 2 pour le génotype homozygote mineur (Tableau \@ref(tab:codingGenotype)).  
La fonction $g(.)$ est la fonction de lien, par exemple, la fonction $logit$ ($log\left(\frac{P(Y=1)}{1-P(Y=1)}\right)$) dans le cas où $Y$ est dichotomique (régression logistique), ou encore la fonction _identité_ lorsque $Y$ est quantitatif (régression linéaire).  
Ces approches sont préférées aux approches basées sur la construction de table de contingence (p. ex. test exact de Fisher), en particulier, dans les études cas/témoins, puisque ces dernières permettent l'ajustement à des covariables ($Z$) cliniques ou démographiques, par exemple.
En outre, l'usage d'un modèle de régression logistique permet de fournir des odds ratios à partir de la mesure de l'effet $\beta_1$ du génotype $X$.

(ref:codingGenotype) Codage du génotype selon le modèle génétique.

```{r codingGenotype, include = TRUE}
CG <- rbind(
    c("AA", "$X=(01)$", "$X=2$", "$X=1$", "$X=1$"),
    c("Aa", "$X=(10)$", "$X=1$", "$X=0$", "$X=1$"),
    c("aa", "$X=(00)$", "$X=0$", "$X=0$", "$X=0$")
)
colnames(CG) <- c("Génotype", "Codominant", "Additif", "Recessif", "Dominant")
knitr::kable(
    CG,
    booktabs = TRUE,
    caption = '(ref:codingGenotype)',
    format = "pandoc",
    align = rep("c", ncol(CG)),
    longtable = FALSE
)
```

Des covariables d'ajustements ($Z$) sont également ajoutées au modèle, particulièrement lorsque le plan d'expérience n'a pas permis de prendre en compte celles-ci.
Malgré un plan d'expérience prenant en compte différents paramètres essentiellement techniques, pouvant être sources de confusion dans l'analyse, il n'est pas toujours possible de tous les contrôler dans un même plan d’expérience. 
Ainsi, ces variables techniques, démographiques et cliniques peuvent être incluses dans le modèle en tant que covariable, par exemple, l'âge, le sexe ou encore l'IMC.
D'autres éventuelles variables peuvent être incluses, comme les premières composantes (deux à cinq) de l'ACP réalisée lors du contrôle-qualité, permettant de prendre en compte une stratification ou mélange (p. ex. ethnique ou géographique) au sein de la population d'étude [@novembre_genes_2008; @clayton_population_2005; @bouaziz_accounting_2011].
L'ajustement pour ces covariables est effectué pour deux raisons principales\ : soit pour éviter de confondre la relation entre le génotype et le phénotype, soit pour réduire la variance résiduelle et ainsi augmenter la précision des estimations.

###### Les approches "longitudinales"
Les études GWA classiques se concentrent sur une seule mesure d'un trait quantitatif par individu pour identifier des variants génétiques, même lorsque des données longitudinales étaient disponibles, principalement à cause de la complexité des modèles permettant d'analyser ces données et en particulier, les temps élevés de calcul.
L'utilisation des modèles linéaires mixtes (LMM) [@laird_random-effects_1982; @liang_longitudinal_1986], des équations estimantes généralisantes (GEE) [@ziegler_generalised_1998], et d'autres approches pour prendre en compte les mesures répétées, est devenue plus fréquente dans les études GWA, avec notamment des groupes de travail (Genetic Analysis Workshop 18) dont la thématique portait sur les méthodes permettant l'analyse des données longitudinales dans un contexte génétique [@almasy_data_2014; @beyene_longitudinal_2014; @wu_mixed-effects_2014].
Les méthodes LMM et GEE permettent de prendre en compte différentes structures de données telles que les données longitudinales et familiales.
Cependant, ce type de données comporte habituellement de nombreuses données manquantes, et nécessite de vérifier certaines hypothèses quant à leur distribution [@graham_missing_2009]\ :

+ MCAR ("missing completely at random")\ : les données sont manquantes indépendamment
des données observées et non observées\ ;

+ MAR ("missing at random")\ : conditionnellement aux données observées, les données
manquantes sont indépendantes des données non observées\ ;

+ MNAR ("missing not at random")\ : les données manquantes sont dépendantes de variables
non observées.

Dans le cas des LMM, les données manquantes doivent être distribuées selon un processus MAR ou MCAR pour obtenir une inférence statistique valide\ ; pour les GEE (inférence non-basée sur la maximisation de la vraisemblance), elles doivent être distribuées selon un processus MCAR [@robins_estimation_1994].
De plus, l'utilisation de données longitudinales implique des hypothèses quant à la structure de corrélation entre les individus et entre les mesures de chaque individu.
Une mauvaise spécification de cette structure de corrélation ou l'omission de ces hypothèses peuvent conduire à un biais dans les estimations [@lu_impact_2009].
L'une des raisons derrière le besoin d'exploiter les données longitudinales, lorsque disponibles, reposent sur le gain de puissance statistique qui peut être obtenu à partir de ces données [@costanza_consistency_2012; @hossain_analysis_2014; @hu_association_2014; @lee_analysis_2014; @wang_comparing_2014; @xu_longitudinal_2014; @zhao_cross-sectional_2014].
Ce gain de puissance est généralement obtenu dans les études GWA classiques par l'augmentation du nombre d'individus de façon directe ou indirecte par méta-analyse.
Pour remédier et contourner le problème de complexité algorithmique induit par le volume des données génomiques, des méthodes dérivées et approchées des LMM [@sikorska_fast_2013; @verbeke_conditional_2001] et des GEE [@robins_estimation_1994; @sitlani_generalized_2015], ainsi que des approches en "deux-étapes" [@hossain_analysis_2014; @houwing-duistermaat_gene_2014; @musolf_mapping_2014; @roslin_genome-wide_2009; @sikorska_gwas_2015; @sikorska_fast_2013; @wang_comparing_2014] ont été développées [@beyene_longitudinal_2014; @wu_mixed-effects_2014; @kerner_use_2009].

Les LMM ont été introduits par @laird_random-effects_1982, et leur forme générale est donnée par l’équation 
`r numberEq.docx("\\boldsymbol{Y}_i=\\beta \\boldsymbol{X}_i + b_i \\boldsymbol{Z}_i + \\epsilon_i", "eq:LMM1")`,
où $\boldsymbol{Y}_i$ représentent les mesures de l'individu $i$ ($i=1, \cdots, n_i$), $\boldsymbol{X}_i$ et $\boldsymbol{Z}_i$ dsignent les matrices respectives des effets fixes et aléatoires.
Ces matrices sont de dimensions respectives $n_i\times p$ et $n_i\times q$, où $p$ et $q$ donnent le nombre de covariables définies en effet fixe et aléatoire.
Les paramètres $\beta$ et $b_i$ désignent les effets fixes et aléatoires pour l'individu $i$ et $\epsilon_i$ dénote un terme d'erreur supposé normalement distribué.
La partie des effets aléatoires peut comporter une ordonnée à l'origine aléatoire, c'est-à-dire que la valeur d'origine de chaque individu peut varier selon l’individu, et inclure une pente aléatoire, ces pentes pouvant varier d'un individu à l'autre.
Lorsque seule l'ordonnée à l'origine est incluse dans les effets aléatoires, la structure de corrélation est dite "compound symmetry", où les corrélations entre les mesures pour un même individu sont constantes dans le temps.
Cette hypothèse est peu réaliste, en particulier dans les données cliniques et épidémiologiques\ ; conséquemment, une pente est généralement incluse dans les effets aléatoires, permettant ainsi d'avoir une structure de corrélation plus complexe, tel qu'un processus autorégressif d'ordre 1 (AR1), par exemple.
Dans un contexte d'étude GWA, le modèle général (en omettant la matrice des covariables) s’écrit sous la forme suivante\ :
`r numberEq.docx("Y_{ij}=\\beta_0+b_{0i}+\\beta_1 t_{ij}+b_{1i} t_{ij}+\\beta_2 G_i+\\beta_3 G_i t_{ij}+\\epsilon_{ij}", "eq:LMM2")`
où $G_i$, le génotype d'un SNP pour l'individu $i$, et $t_{ij}$ désigne le temps de la mesure $j$ de l'individu $i$.
Néanmoins, en raison de la complexité computationnelle, ces modèles sont habituellement appliqués sur des ensembles réduits de SNPs (p. ex. chromosome, gènes ou SNPs candidats), et parfois en omettant le terme représentant la pente aléatoire [@hu_association_2014; @wu_mixed-effects_2014; @mei_longitudinal_2012; @lee_analysis_2014; @liu_penalized_2014; @xu_longitudinal_2014; @smith_longitudinal_2010].

Une approche dérivée des LMM a également été proposée sous le nom de LMM conditionnel (cLMM) [@verbeke_conditional_2001].
Ce modèle, en plus d'estimer les effets longitudinaux (c.-à-d. les effets sur l'ensemble des mesures d'un individu), garantit une plus grande robustesse relativement à une mauvaise spécification des caractéristiques de la mesure à l'origine (c.-à-d. la première mesure d'un individu).
Le cLMM peut s'écrire, à partir de l'équation `r labEq.docx("LMM1")`\ : 
`r numberEq.docx("\\boldsymbol{Y}_i=\\beta^1 \\boldsymbol{X}_i^1+\\beta^2 \\boldsymbol{X}_i^2+b_i^1 \\boldsymbol{Z}_i^1+b_i^2 \\boldsymbol{Z}_i^2+\\epsilon_i", "eq:cLMM")`
Dans ce modèle, les effets fixes ($\boldsymbol{X}_i$) et aléatoires ($\boldsymbol{Z}_i$) sont décomposés respectivement en\ : 

+ $\boldsymbol{X}_i=(\boldsymbol{X}_i^1|\boldsymbol{X}_i^2)$, où $\boldsymbol{X}_i^1$ représente la matrice des covariables indépendantes du temps ($n_i\times p_1$) et $\boldsymbol{X}_i^2$ la matrice des covariables dépendantes du temps ($n_i\times p_2$)\ ;

+ $\boldsymbol{Z}_i=(\boldsymbol{Z}_i^1|\boldsymbol{Z}_i^2)$, où $\boldsymbol{Z}_i^1=\boldsymbol{1}_{(n_i)}$ et $\boldsymbol{Z}_i^2$ représente la matrice des covariables dépendantes du temps ($n_i\times (q-1)$).

L’évolution temporelle des traits cliniques mesurés aux différentes visites peut être modélisée par une droite, ce qui a motivé les approches dites en "deux étapes". 
Elles consistent à utiliser un modèle "simplifié", c’est-à-dire sans la variable d'intérêt (SNP testé) en premier lieu, puis utiliser l'un des paramètres estimés dans un second modèle en incluant la variable d'intérêt, par exemple, et en prenant comme variable réponse la pente [@sikorska_fast_2013], l'ordonnée à l'origine [@wang_sample_2014] ou les résidus [@hossain_analysis_2014].
Le modèle s’écrit alors\ :
`r numberEq.docx("Y_{ij}=\\beta_{0i}^\\Delta+\\beta_{1i}^\\Delta t_{ij}+\\epsilon_{ij}^\\Delta", "eq:LMMshort")`

Enfin, certaines approches utilisent des modèles à classes latentes en "deux étapes" [@roslin_genome-wide_2009; @musolf_mapping_2014].
L'idée principale consiste à réduire l'information du trait quantitatif à un trait qualitatif reposant, par exemple, sur les probabilités bayésiennes a posteriori (probabilité de chaque individu d'appartenir à un groupe ou sous-groupe), et d’inclure ces probabilités comme variables réponses dans un second modèle.
Cette méthode possède l’avantage de réduire le temps de calcul en diminuant leur complexité.

Une alternative aux LMM est l'approche GEE [@liang_longitudinal_1986].
Il s'agit d'une méthode semi-paramétrique dont l'objectif est l'inférence de l'effet moyen sur la population d'une variable d'intérêt.
Cette méthode nécessite de vérifier d’abord la nature de la distribution des données manquantes, ainsi que de définir la "bonne" structure de corrélation des mesures intra-individuelles.
Elle peut s'écrire, dans sa formulation la plus simple, comme\ :
`r numberEq.docx("E(\\boldsymbol{Y}_{it})=\\beta_0+\\beta_1 \\boldsymbol{X}_{it}+ \\gamma \\boldsymbol{Z}_{it}", "eq:GEE")`
Contrairement aux approches LMM, les GEE nécessitent que les données manquantes soit distribuées selon un processus MCAR. 
Des améliorations ont été proposées pour réduire l'impact du non-respect de cette hypothèse, et rendre de ce fait les GEE plus robustes aux distributions des données manquantes [@robins_analysis_1995; @robins_estimation_1994].
Cependant, dans le contexte des études GWA, la violation de cette hypothèse pourrait ne pas être un problème, puisque les données manquantes ne peuvent s’expliquer par un seul variant génétique avec effet faible [@sitlani_generalized_2015].

Les données longitudinales offrent, en plus de pouvoir modéliser l'évolution de différents traits quantitatifs, la possibilité d'étudier la survenue d'un événement, en particulier le développement d'une pathologie comme le diabète de type 2. 
Dans ce contexte, les études GWA ont dans un premier temps réalisé des tests d'associations au moyen d’une régression logistique\ ; cependant, cette approche ne permet pas de prendre en compte la composante longitudinale et se limite à une seule mesure du trait. 
Les modèles de survie, tel le modèle de Cox, se présentent comme des alternatives au modèle de régression logistique. 
Ces approches commencent à se développer et à être optimisées pour application sur données réelles en génétique [@syed_evaluation_2016; @syed_survivalgwas_power:_2016].

La modélisation conjointe permet de modéliser d'une part, la composante longitudinale (c.-à-d. la trajectoire de la variable étudiée) et d'autre part, la composante de survie (c.-à-d. la survenue de l'événement étudié). 
La composante longitudinale consiste typiquement à l'application d'un modèle linéaire mixte\ :
`r numberEq.docx("Y_{ij}=X_{ij}+\\epsilon_{ij}", "eq:eq1")`
où $Y_{ij}$ est la valeur observée et $X_{ij}$ la vraie (non observée) valeur de la variable longitudinale. 
Le terme $\epsilon_{ij}$ est le terme d'erreur aléatoire supposé distribué selon la loi Normale\ :
`r numberEq.docx("\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)", "eq:eq2")`
La quantité $X_{ij}$ représente la fonction de la trajectoire et est définie usuellement comme une fonction linéaire (ou quadratique) dépendante du temps. 
Des covariables peuvent aussi être incluses dans cette fonction, comme l'âge, le sexe ou l'IMC. 
Par exemple, si $Y_{ij}$ représente les valeurs mesurées de la glycémie à jeun au temps $t_{ij}$, $Z_i$ désigne le génotype du SNP analysé pour l'individu $i$, et $W_i$ désigne les covariables, le modèle s'énonce\ :
`r numberEq.docx("Y_{ij}=X_{ij}+\\epsilon_{ij}=\\theta_{0i}+\\theta_{1i}\\times t_{ij}+\\gamma \\times Z_i+\\delta \\times W_i + \\epsilon_{ij}", "eq:eq3")`
Pour simplifier l'écriture dans ce qui suit, le terme $\delta \times W_i$ sera omis. 
Les paramètres $\theta_{0i}$ et $\theta_{1i}$ sont réputés distribués selon une distribution Normale bivariée\ :
`r numberEq.docx("\\theta \\sim \\mathcal{N}_2(\\mu, \\boldsymbol{\\Sigma})", "eq:eq4")`
Le paramètre $\gamma$ évalue l'effet de $Z_i$ (p. ex. effet additif du SNP) sur la fonction de la trajectoire.
Pour tenir compte éventuellement de pentes différentes entre les génotypes, un terme d'interaction entre $Z_i$ et le temps peut être inclus dans la fonction de la trajectoire.  

La composante de survie (p. ex. survenue du DT2) se compose généralement d'un modèle paramétrique (p. ex. Exponentielle ou Weibull) ou semi-paramétrique (p. ex. risques proportionnels de Cox) avec\ :
`r numberEq.docx("h_i(t)=h_0(t) \\exp(\\beta X_i(t)+\\alpha Z_i)", "eq:eq5")`
où $h_i(t)$ est la fonction de risque au temps $t$ pour l'individu $i$, et $h_0(t)$ est la fonction de risque de base non spécifiée (Tableau \@ref(tab:surv)). 
Le coefficient $\alpha$ mesure l'effet de $Z_i$ sur le temps de survenue de l'événement, alors que le coefficient $\beta$ mesure l'association entre la trajectoire $Y_i$ et le temps de survenue.

Le choix d'un modèle joint est généralement motivé par la volonté de modéliser le lien entre les variables d'intérêt de chaque composante (c.-à-d. variable longitudinale et variable événement), d'évaluer le mécanisme sous-jacent aux données manquantes, et de prendre en compte une covariable dépendante du temps [@sudell_joint_2016].
Les modèles principalement employés dans cette approche sont les modèles linéaires mixtes et les modèles de Cox, ceux-ci étant déjà implémentés au sein d'extensions du logiciel R [@R-JM; @R-joineR; @R-JMbayes]. \clearpage

(ref:surv) Caractéristiques des distributions exponentielle, de Weibull et de Gompertz sous le modèle de Cox avec $h(t)=h_0(t) \exp(\beta X)$.

```{r surv, include = TRUE}
surv <- rbind(
  c("Paramètre d'échelle", "$\\lambda >0$", "$\\lambda >0$", "$\\lambda >0$"),
  c("Paramètre de forme", "", "$\\nu >0$", "$-\\inf<\\alpha < \\inf$"),
  c("Fonction de risque", "$h_0(t)=\\lambda$", "$h_0(t)=\\lambda\\nu t^{(\\nu-1)}$", "$h_0(t)=\\lambda\\exp(\\alpha t)$"),
  c("Fonction de risque", "$H_0(t)=\\lambda t$", "$H_0(t)=\\lambda t^\\nu$", "$H_0(t)=\\frac{\\lambda}{\\alpha}(\\exp(\\alpha t)-1)$"),
  c("cumulée", "", "", ""),
  c("Fonction inverse", "$H_0^{-1}(t)=\\lambda^{-1} t$", "$H_0^{-1}(t)=(\\lambda^{-1} t)^{(1/\\nu)}$", "$H_0^{-1}=\\frac{1}{\\alpha}\\log(\\frac{\\alpha}{\\lambda}t+1)$"),
  c("de risque cumulée", "", "", ""),
  c("Temps d'événement", "$T=-\\frac{\\log(u)}{\\lambda\\exp(\\beta X)}$", "$T=\\left(-\\frac{\\log(u)}{\\lambda\\exp(\\beta X)}\\right)^{(1/\\nu)}$", "$T=\\frac{1}{\\alpha}\\left(1-\\frac{\\alpha\\log(u)}{\\lambda\\exp(\\beta X)} \\right)$"),
  c("($u\\sim Uniforme(0,1)$)", "", "", "")
)
colnames(surv) <- c("Caractéristique", "Exponentielle", "Weibull", "Gompertz")
knitr::kable(
  surv,
  booktabs = TRUE,
  caption = '(ref:surv)',
  format = "pandoc",
  align = rep("c", ncol(surv)),
  longtable = FALSE
)
```

###### Correction post-analyse
(ref:lambda) Graphique quantile-quantile des valeurs-p de l'étude d'association épigénétique en cas/contrôle sur le diabète de type 2 (Chapitre \@ref(Article3)), avant et après normalisation sur le facteur d'inflation $\lambda$. La ligne horizontale en pointillé représente le seuil de significativité corrigé selon la méthode de Bonferroni.

```{r lambda, fig.cap = '(ref:lambda)', out.width = '3in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/Figure1_A.png")
```

Une fois l'analyse réalisée au moyen de logiciels tels que PLINK [@chang_second-generation_2015; @purcell_plink_2015], SNPTEST [@burton_genome-wide_2007; @clark_conjuring_2007] ou d'extensions du logiciel R [@huber_orchestrating_2015], une correction appelée contrôle génomique ("Genomic control") peut être appliquée, avec comme principale motivation la correction d’une éventuelle stratification ou mélange au sein de la population d'étude. 
Cette inflation est mesurée par le paramètre de sur-dispersion $\lambda$ sur l’ensemble des statistiques de test observées [@devlin_genomic_1999].
Il est estimé à partir de la distribution observée des m statistiques de test de chi-deux comparée à celle attendue sous l'hypothèse nulle. 
Pour se prémunir des valeurs extrêmes, la valeur médiane (et non la moyenne) est utilisée
 $\lambda=\frac{median(\chi^2_1, \cdots, \chi^2_m)}{0,4549}$.
La distribution de la statistique de test observée dans l'étude est ensuite corrigée par ce facteur $\lambda$ (Figure \@ref(fig:lambda)).
Il est à noter que même si le paramètre $\lambda$ n'est pas directement relié à la fréquence allélique, cette correction peut toutefois engendrer une perte de puissance [@georgiopoulos_power_2016]

Les études GWA consistant en la réalisation d'un grand nombre de répétitions d'un même test statistique, il convient d’appliquer une correction pour test multiples afin de diminuer le taux de faux-positifs global de l'étude [@pearson_how_2008].
Un test statistique est dit significatif, et son hypothèse nulle rejetée, si la valeur-p est inférieure à un seuil $\alpha$ déterminé en amont de l'analyse (communément fixé à 0,05). 
En d'autres mots, on admettra que l'hypothèse nulle est rejetée à tort dans 5\ % des répétitions (taux de faux-positifs). 
Ce seuil de 5\ % est applicable sur un seul test statistique\ ; pour un grand nombre de tests, la probabilité cumulée d'observer un ou plusieurs faux-positifs augmente avec le nombre de tests. 
Depuis la première étude GWA, la méthode de correction du seuil $\alpha$, choisie préférentiellement en raison de sa simplicité, est la méthode de Bonferroni [@dunn_multiple_2012].
Cette méthode suppose que les tests effectués sont indépendants entre eux, et consiste à diviser le seuil $\alpha$ par le nombre de tests.
Par exemple, pour 500\ 000 tests à un seuil de 5\ %, on obtient $\alpha_{cor}=0,05/(500\,000)=1\times 10^{-7}$.
Cependant, l'hypothèse d'indépendance n'est pas vérifiée en raison de la présence connue de déséquilibre de liaison entre les SNPs étudiés. 
D'autres méthodes de correction existent et ont été utilisées, tels le FDR ("False Discovery Rate") qui consiste à estimer le taux de faux-positifs parmi tous les résultats significatifs à un seuil pré-défini $\alpha$ [@hochberg_more_1990; @van_den_oord_controlling_2008], ou encore les méthodes de permutation comme celles développées au sein du logiciel PLINK [@chang_second-generation_2015; @purcell_plink_2015]. 
À l’heure actuelle, le seuil nominal communément admis de significativité est $\alpha_{cor}=5\times 10^{-8}$.
Ce seuil de significativité pangénomique ("genome-wide significance"), a été établi en considérant une étude avec individus d'origine caucasienne [@dudbridge_estimation_2008] et réalisée à une échelle pangénomique d’environ 1 million de SNPs indépendants.

Enfin, les résultats d'une étude GWA, après la découverte de nouveaux loci, peuvent faire l'objet d'une validation ou d’une réplication, c'est-à-dire que le même modèle (ou suffisamment proche) est appliqué dans une population indépendante à celle de l'étude initiale, mais dont les caractéristiques populationnelles sont similaires. 
Pour qu'un résultat soit validé, certaines caractéristiques doivent être obtenues dans l'étude de réplication, telles\ :

+ une taille d’échantillon suffisamment grande avec puissance statistique estimée à au moins 80\ %, permettant ainsi de limiter le taux de faux-négatifs et d’augmenter la réplication des "vrais-positifs" de l'étude initiale\ ;

+ un effet dans la même direction (p. ex. effet positif), présentant une valeur-p inférieure au seuil nominal $\alpha$ de 5\ %.

Ces études de réplication, même si elles ne sont pas obligatoires, permettent tout de même de réduire le nombre de faux-positifs rapportés dans les résultats. 
Une autre approche de validation concerne les méta-analyses [@zeggini_meta-analysis_2009; @evangelou_meta-analysis_2013].
Ces méthodes permettent, soit en agrégeant les valeurs-p (méthode de Fisher) de plusieurs études (l'étude initiale n'étant pas incluse pour éviter d'orienter les résultats), soit en utilisant les effets estimés et leur erreurs-types (permettant de pondérer les effets selon la taille de l’échantillon de l'étude), de proposer une valeur-p globale et transversale à plusieurs études, indiquant par le fait même si le résultat de l'étude initiale est validé ou non. 
L'hétérogénéité des études incluses dans la méta-analyse doit être prise en compte afin d’éviter de tirer des conclusions erronées créées principalement par les différences entre celles-ci.


#### Transcriptomique
Une fois complété le pré-traitement et le contrôle-qualité des mesures de fluorescence adaptés à la plateforme utilisée (p. ex. Affymetrix, Illumina ou Agilent), l'approche classique consiste à identifier des gènes différentiellement exprimés selon une ou plusieurs conditions expérimentales.
En effet, l'objectif premier des puces d'expression est d'identifier des processus biologiques ou voies biologiques permettant de discriminer deux ou plusieurs groupes. 
Les différentes puces permettent de quantifier l'expression (quantité de mRNA) pour l'ensemble des gènes sur le génome et les différents transcrits de ces gènes. 
Cependant, d'une plateforme à une autre, les informations sur les transcrits diffèrent aussi bien en termes de quantification que d'annotation des sondes, c.-à-d. la localisation des sondes par rapport aux transcrits. 
Il est donc difficile de comparer les mesures d'expression entre des plateformes différentes.

La détection de gènes différentiellement exprimés s'effectue généralement au moyen d'un modèle de régression linéaire généralisé appliqué à chaque gène individuellement. 
En plus de proposer un cadre commun d'analyse des données issues des différentes plateformes [@R-limma], l'extension R _limma_ propose une amélioration du modèle en incluant, d'une part, une composante hiérarchique permettant de prendre en compte la variabilité inter-puce, et d'autre part, une modification de la statistique de test en régulant l'erreur-type de l'estimateur par un paramètre de variance de chaque gène sur l'ensemble des puces et des échantillons (paramètre estimé par une approche bayésienne empirique). 
Cette amélioration accroît la stabilité de l'inférence comparée à celle de l'approche classique obtenue par régression linéaire [@smyth_linear_2004; @phipson_robust_2016].
Afin d'identifier les gènes différentiellement exprimés, le seuil de significativité $\alpha$ doit être défini en considérant le nombre de tests réalisées, par exemple, au moyen d'une correction sur le FDR (option par défaut dans (option par défaut dans _limma_).

(ref:networkIPA) Identification par IPA d'un réseau de gènes associé à la diminution de l'expression de _ZFAND3_ (Chapitre \@ref(Article2)).

```{r networkIPA, fig.cap = '(ref:networkIPA)', out.width = '3in', out.height = '3.6in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/networkIPA.png")
```

À l'issue de l'analyse, un grand nombre de gènes peut alors être différentiellement exprimé. 
A priori, il est difficile d'identifier parmi ces gènes, une fonction ou une voie métabolique commune qui pourrait éclairer sur l'aspect biologique de ces résultats. 
Afin de réduire l'information et identifier des groupes de gènes impliqués dans une fonction particulière, une approche consiste à effectuer des tests d'enrichissement, c'est-à-dire à évaluer si l'ensemble des gènes présente en excès un groupe de gènes particuliers liés à une fonction ou à une localisation dans un tissu particulier. 
Ces études d'enrichissement peuvent se faire à partir de différentes bases de données telles que "Gene Ontology" [@ashburner_gene_2000], KEGG ("Kyoto Encyclopedia of Genes and Genomes") [@kanehisa_kegg:_2017] ou encore depuis des logiciels d'analyse ayant accès à leurs propres bases de connaissance comme "Ingenuity Pathway Analysis" (IPA) (Figure \@ref(fig:networkIPA)) et "Gene Set Enrichment Analysis" (GSEA) [@subramanian_gene_2005].  
Une autre approche consiste à utiliser des méthodes de classification hiérarchique représentées sous la forme de "heatmap" comportant deux dendrogrammes, l'un représentant les dissimilarités entre les échantillons, et l'autre celles entre les gènes/transcrits (Figure \@ref(fig:heatmap)).
Cette méthode permet de visualiser rapidement et simplement les groupes de gènes différentiellement exprimés entre plusieurs conditions expérimentales.

(ref:heatmap) Représentation "heatmap" des micro ARN différentiellement exprimés entre 10 nM et 10 $\mu$M, dans les 3 bisphénols (A, F et S). Les valeurs représentent les $\log_2$ des "Fold Change" (c.-à-d. le ratio de l'expression d'un miRNA, pour un bisphénol donné, sur la condition contrôle, sans exposition bisphénol) [@verbanck_low-dose_2017].

```{r heatmap, fig.cap = '(ref:heatmap)', out.width = '2.5in', out.height = '5in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/heatmap.png")
```


#### Méthylomique
Après le pré-traitement des données provenant des puces de méthylation (c.-à-d. après filtrage des sondes problématiques et normalisation des différents biais techniques), il est possible d'analyser ces données généralement obtenues selon un plan d'expérience de type cas/témoins.

Lorsque le plan d'expérience le permet, c'est-à-dire lorsque les individus sont appariés entre cas et témoins, un test-t ou un test non-paramétrique de Mann-Whitney peut être appliqué à chaque sonde. 
Dans le cas contraire, les mêmes covariables d'ajustement que dans les études GWA sont incluses dans un modèle de régression (logistique ou linéaire), à savoir l'âge, le sexe, l'IMC, voire des composantes principales. 
Une sonde est une position différentiellement méthylée ("Differentially Methylated Position", DMP) lorsque la valeur-p du test est inférieure au seuil nominal de $\alpha=0,05$ ou corrigé pour les tests multiples (c.-à-d. Bonferroni, FDR, etc.).
Les données extraites depuis le logiciel GenomeStudio représentent les niveaux de méthylation mesurés à chaque sonde, et rapportés en tant que valeur-$\beta$ (suit une distribution Bêta sous l'hypothèse que les intensités sont distribuées selon une loi Gamma) définie comme $\beta=\frac{M}{(M+U+\phi)}$, où $M$ est l'intensité de l'allèle méthylé, $U$ est l'intensité de l'allèle non-méthylé et $\phi$ une constante pour compenser des intensités sont faibles.
Les valeurs-$\beta$ varient de zéro (complètement non-méthylé) à un (complètement méthylé).
Les caractéristiques de la distribution de ces valeurs n'en font pas une bonne variable pour les tests statistiques (p. ex. test-t et régression linéaire) qui s’appuient sur l'hypothèse d'homoscédasticité de la variable (variance constante). 
La transformation des valeurs-$\beta$ en valeurs-$M$ ($M=log_2\left(\frac{M+\phi}{U+\phi}\right)$) a été proposée et étudiée pour contourner cet écueil [@du_comparison_2010].
En raison de l'interprétation biologique simple des valeurs-$\beta$, lesquelles sont exprimées en pourcentages, mais de la "qualité" statistique des valeurs-$M$, les analyses statistiques sont réalisées sur les valeurs-$M$, et ce sont les valeurs-$\beta$ qui sont reportées.

(ref:DMR) Représentation de la méthylation du gène _PDGFA_ et du locus CpG cg14496282 en présence d'insuline (Chapitre \@ref(Article3)).

```{r DMR, fig.cap = '(ref:DMR)', out.width = '6in', out.height = '4.8in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/DMR.png")
```

Une seconde approche peut être réalisée en considérant non plus chaque sonde de façon individuelle, mais en considérant des régions ou blocs, permettant de ce fait d'identifier des régions différentiellement méthylées ("Differentially Methylated Region", DMR) [@peters_novo_2015; @hansen_increased_2011; @hansen_bsmooth:_2012] (Figure \@ref(fig:DMR)).
Cette approche se base sur l'hypothèse que des sondes dans un même voisinage auraient le même comportement en termes de méthylation\ : par exemple, une région hypométhylée ou méthylée au niveau du promoteur de la transcription d'un gène. 
Cependant, en raison de la couverture relativement faible et non-homogène du génome par la puce Illumina HumanMethylation450 [@bibikova_high_2011], cette approche ne permet d'étudier que partiellement ces régions, et est destiné principalement aux techniques de séquençage (p. ex. séquençage bisulfite) ou aux puces bénéficiant d'une plus grande couverture, comme c'est le cas dans les dernières puces Illumina HumanMethylation850 [@moran_validation_2016].

Comme dans les études GWA, une validation ou une réplication des résultats est préconisée d'autant plus que les biais techniques (p. ex. types de sonde, effet plaques, etc.) représentent des facteurs majeurs menant à une inflation du taux de faux-positifs. 
Une première approche consiste à réaliser une réplication avec la même technologie sur une cohorte indépendante présentant des caractéristiques similaires. 
Une seconde approche consiste à réaliser une validation d'un DMP ou DMR à l'aide d'une autre technologie (p. ex. mesurer la méthylation d'un DMP/DMR via une technique ciblée comme le pyroséquençage) sur la même population que l'étude initiale [@kurdyukov_dna_2016].
Cette dernière présente l'avantage de pouvoir valider ce qui a été observé dans une population donnée avec une technologie donnée, et permet en conséquence de réduire le risque de faux-positifs imputable à un facteur technique, mais ne permet toutefois pas d'éliminer une potentielle spécificité de la population étudiée (p. ex. biais de sélection). 
En revanche, la première approche permet de contrôler à la fois ces deux phénomènes, lorsque la population de réplication est adaptée.

(ref:compcell) Estimation des différences de composition cellulaire (selon une base de référence) entre le groupe diabétique et le groupe contrôle.

```{r compcell, fig.cap = '(ref:compcell)', out.width = '3in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/Figure8.png")
```

Il est à noter que la méthylation diffère entre les tissus, et plus précisément entre les types cellulaires. 
Les échantillons provenant généralement de prélèvements sanguins, voire de biopsies, la composition cellulaire du prélèvement, c.-à-d. le nombre et la proportion des types cellulaires qui composent le tissu, peut ne pas être homogène d'un individu à l'autre, et par conséquent, d'un groupe à l'autre. 
Ceci peut représenter un facteur de confusion avec le statut cas/témoin d’une étude. 
Il convient alors de considérer l’étude des valeurs-$\beta$ à l'aide d'une base de référence [@houseman_dna_2012; @teschendorff_comparison_2017; @cardenas_validation_2016] (Figure \@ref(fig:compcell)) ou de façon algorithmique, comme s’il s’agissait d’un mélange de distributions, par exemple, ou en effectuant une décomposition de celles-ci sur le même principe qu'une ACP [@houseman_cell-composition_2015; @houseman_reference-free_2016; @houseman_reference-free_2014].
Cette différence potentielle de composition cellulaire peut également être corrigée de la même façon que dans les études GWA, au regard de la stratification de la population, avec notamment l'application d'une correction post-analyse comme le contrôle génomique, simultanément ou en plus de sa prise en compte en tant que covariable d'ajustement dans le test d'association.


#### Multi-omique
Lorsque les données de différentes omiques sont disponibles pour un ensemble d'individus, il peut être intéressant d'intégrer ces différentes sources d'information afin d’améliorer et de parfaire la compréhension d'une association mise en avant lors des analyses décrites précédemment. 
Lorsqu'un gène/SNP/CpG est identifié comme candidat pour la susceptibilité au trait d'intérêt, des analyses complémentaires et ciblées peuvent être réalisées et pourront être étudiées de façon conjointe. 
La méthode employée le plus souvent consiste alors soit à mesurer les corrélations entre les différentes données omiques, soit à inclure cette information additionnelle en tant que covariable dans le modèle de régression. 
Lorsque des puces ADN sont employées et qu'aucune hypothèse ou sélection a priori n'est réalisée, le volume de données devient très important et difficle à intégrer pour l'ensemble des trois omiques discutées dans ce manuscrit (Tableau \@ref(tab:omics)).
Quelques méthodes exploitant deux omiques différentes seront rapidement détaillées dans la suite. 
Heureusement, des approches de type "machine learning" commencent à voir le jour pour analyser tous les types de données omiques simultanément [@lin_machine_2017], données provenant de puces ou issues de méthodes de séquençage à haut-débit dit de nouvelle génération (_Next Generation Sequencing_).

(ref:omics) Plateformes omiques (puce-à-ADN) utilisées dans les Chapitres \@ref(Article1), \@ref(Article3) et \@ref(Article4).

```{r omics, include = TRUE}
dtaomics <- readxl::read_excel(path = "FiguresTables/omics.xlsx")
knitr::kable(
    dtaomics,
    booktabs = TRUE,
    caption = '(ref:omics)',
    format = "pandoc",
    align = rep("c", ncol(dtaomics)),
    row.names = FALSE,
    longtable = FALSE
)
```

\clearpage

##### eQTL\ : "expression Quantitative Trait Loci"
Quoique les études GWA aient permis d'identifier des loci de susceptibilité, le mécanisme reliant ces SNPs à la maladie n'est pas évident. 
Les analyses dites eQTL pour "expression Quantitative Trait Loci" [@rockman_genetics_2006; @gibson_quantitative_2005] proposent d'intégrer l'information d'expression des gènes à ces loci de susceptibilité de façon à identifier une relation de causalité entre génomique et transcriptomique [@rockman_genetics_2006; @gibson_quantitative_2005].
Ces analyses eQTL considèrent la mesure d'expression d'un gène/transcrit en tant que trait quantitatif sur lequel le génotype d'un SNP, ainsi que des covariables, seront régressés pour évaluer la relation entre le génotype et l'expression. 
Les combinaisons de SNPs (environ 1\ 000\ 000) et de gènes (environ 20\ 000), pouvant aboutir à un très grand nombre de configurations possibles ($\simeq10^{10}$), ces analyses soulèvent notamment des problèmes computationnels [@mackay_genetics_2009].

(ref:eQTL) Distribution des valeurs-p d'association entre les SNPs et l'expression des gènes à moins de un mégabase du promoteur du gène.

```{r eQTL, fig.cap = '(ref:eQTL)', out.width = '4.8in', out.height = '2.4in', include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/eQTL.png")
```

Les tests peuvent être effectués en _cis_, c'est-à-dire qu'un gène est testé pour un SNP si celui-ci est localisé à moins d'une certaine distance en paires de bases (en général, entre 100 kb et 1 Mb) du gène, ou en _trans_ lorsque le SNP excède la distance définie [@rockman_genetics_2006; @cheung_genetics_2009].
Une façon de définir la distance est de réaliser l'analyse sur un sous-ensemble (aléatoire) des combinaisons à tester et d'étudier la répartition du signal selon la distance en paire de base (Figure \@ref(fig:eQTL)).
Les SNPs en _trans_ ne sont, en raison de contraintes computationnelles ou d'interprétations difficiles, généralement pas analysés. 
En effet, l'hypothèse courante suppose qu'un SNP a une probabilité plus élevée de produire un effet sur l'expression d'un gène s'il se trouve dans le corps du gène, ou en amont, c'est-à-dire proche des promoteurs de la transcription ("Transcription Start Site" ou TSS).
Par extension, cette approche eQTL est applicable aux données d'épigénomique et connue sous le nom de meQTL ("methylation Quantitative Trait Loci") ou mQTL (utilisé parfois comme "metabolomic Quantitative Trait Loci"), avec en lieu et place de l'expression du gène, le niveau de méthylation d'un site CpG.

##### Causalité\ : la randomisation mendélienne

(ref:MR) Représentation schématique de la randomisation mendélienne. La randomisation mendélienne peut être utilisée pour tester l'hypthèse selon laquelle $Y$ est causale de $X$ selon que les conditions (1), (2) et (3) soient remplies par la variable instrumentale $Z$, où (1) $Z$ est associée à $Y$, (2) $Z$ n'est pas associée à $X$, et (3) $Z$ n'est pas associé à des facteurs de confusions mesurés ou non.

```{r MR, fig.cap = '(ref:MR)', out.width = "6in", include = TRUE, echo = FALSE}
  knitr::include_graphics(path = "FiguresTables/MR.png")
```

En combinant les résultats des études GWA et eQTL, il est possible d'évaluer la causalité d'un SNP sur une pathologie en considérant que, si un SNP est associé au risque de développement d'une pathologie et que ce même SNP est également associé à des changements de l'expression d'un gène (_cis_), alors il est probable que le SNP soit causal de la pathologie [@nica_candidate_2010; @schadt_integrative_2005; @zhu_increasing_2007; @chang_genome-wide_2016].
Cette idée sous-tend la randomisation mendélienne [@lawlor_mendelian_2008], où l'expression d'un gène peut être considérée comme un phénotype intermédiaire ($Y$) entre le génotype (variable instrumentale, $Z$) et le phénotype d'intérêt ($X$) (p. ex. statut diabétique, glycémie, insulinémie, etc.) (Figure \@ref(fig:MR)).

En général, même s'il est possible de tester l’association entre deux variables ($X$ et $Y$), il n'est pas possible de répondre quant à la direction de cette relation\ : cause ou conséquence ?
En outre, l'association entre ces deux variables peut être le résultat de facteurs de confusion, mesurés ou non, ayant un effet sur une ou les deux variables, causant un biais dans l'estimation de cette association. 
Pour évaluer si la variable intermédiaire $Y$ est causale de la variable d'intérêt $X$, la randomisation mendélienne consiste à utiliser une variable instrumentale $Z$, répondant aux conditions suivantes\ : 1) $Z$ est associée à la variable intermédiaire $Y$, 2) $Z$ ne présente pas d'association (directe) avec la variable d'intérêt $X$ et 3) $Z$ ne présente pas d'association avec des facteurs de confusion (mesurés ou non) (Figure \@ref(fig:MR)).
Le génotype d'un SNP ou un score agrégé de génotypes (p. ex. score de risque génétique) remplit ces conditions et permet de déduire la direction de la causalité, puisque la génétique exerce un effet partiel ou total sur les traits biologiques, et non l'inverse. 
De plus, la mesure du génotype n'est soumise qu'à peu de variabilité et de biais. 
Grâce à la structure de LD entre les variants, il n'est pas obligatoire d'utiliser le SNP causal du trait\ ; un variant en fort LD avec celui-ci peut être utilisé de façon équivalente.
